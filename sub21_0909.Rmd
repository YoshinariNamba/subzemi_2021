---
title: "2021年度 サブゼミ 第14回 テキストマイニング"
author: "Yoshinari Namba"
date: "2021/9/9"
output: 
  github_document:
    pandoc_args: --webtex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# 0. イントロダクション

## テキストマイニングとは?
テキストマイニング (text mining) は文字通り，テキストデータから意味のあるパターンやインサイトを探索する分析のことです．
マーケティングの領域ではアンケートの自由回答欄を分析する際に使われているほか，人文科学の分野でも書き手の癖を見つけたり，作者未詳の作品の書き手を判別する際に利用されているそうです．

## Agenda
今日のAgendaは次の通りです.  
1. テキストデータの集計・可視化  
2. 共起語  
3. 対応分析  
4. 回帰分析への応用  
  
大まかな分析からより詳細な分析にステップアップする流れで進めていきます．

## セットアップ
テキストマイニングを始める前に分析環境を整えます．

### 準備1: MeCabのインストール
[これ](http://taku910.github.io/mecab/#download)に沿ってMeCabをダウンロードしてください．

### 準備2: Rプロジェクト
RStudioを開いて右上のタブの File -> New Project から新規プロジェクトを作成しておいてください．

### 準備3: データ
今日のデータは`jalan.csv`を使用します．これは宿泊予約サイトじゃらんにリスティングされている[「季の湯 雪月花」の口コミページ](https://www.jalan.net/yad371898/kuchikomi/?screenId=UWW3001&stayCount=1&yadNo=371898&dateUndecided=1&roomCount=1&roomCrack=200000&adultNum=2&callbackHistFlg=1&smlCd=141602&distCd=01&activeSort=5)をスクレイピングしたものです．  
Rプロジェクトが作成出来たら以下のコードを実行してください．
```{r eval = FALSE}
df_jln <- read.csv("jalan.csv", fileEncoding = "Shift-JIS")

# 確認
View(df_jln)
```

```{r echo=FALSE}
df_jln <- read.csv("./data/jalan.csv", fileEncoding = "Shift-JIS")
```

### 準備4: パッケージ
以下のコードを実行して，今日使用するパッケージをインストール&呼び出しします．
(最初の2つはインストール済みだと思います.)
```{r eval=FALSE}
# インストール
install.packages("RMeCab")
install.packages("tidyverse")
install.packages("magrittr")
install.packages("wordcloud")
install.packages("igraph")
install.packages("FactoMineR")
install.packages("factoextra")

# 呼び出し
library(tidyverse)
library(RMeCab)
library(magrittr)
library(wordcloud)
library(igraph)
library(FactoMineR)
library(factoextra)
```

```{r echo = FALSE}
library(tidyverse)
library(RMeCab)
library(magrittr)
library(wordcloud)
library(igraph)
library(FactoMineR)
library(factoextra)
```


# 1. テキストデータの集計・可視化

## 1.1 ざっくり見てみる
まずはデータを確認します．`summary()`を使うのが定石でしたね．
```{r}
df_jln %>% summary()
```
なんか微妙ですね．データの型が`character`だと`summary()`から有益な情報が得られません．特に`gender`と`age`は「男性or女性」だったり「20代, 30代...」というカテゴリカル変数になっているので，それぞれに当てはまる人数をカウントしてほしいですよね．`table()`を使う方法もありますが，ここでは`dplyr::mutate_at()`という関数を使って`gender`と`age`の型を`factor`型に直し，一気に分布を確認しましょう.  
型の変換には`as.factor()`を使っており，`mutate_at()`の`.funs =`で指定しています.

```{r}
df_jln %>% 
  mutate_at(.vars = vars(gender, age), 
            .funs = as.factor) %>% 
  summary()
```

これで男女，各年代についてそれぞれ何人ずついるのかわかりましたね．


## 1.2 .txtファイルの出力
`summary()`を見たのでさっそく分析に入っていきたいところですが，もう少し準備が必要です．  
スクレイピングなどで取得するデータの多くはデータフレームの形をしていますが，実はテキストマイニングに必須の`RMeCab`ライブラリに入っている関数の多くは`.txt`のファイルを直接読み込む仕様になっています．つまり，データフレームに含まれているテキストデータを`.txt`ファイルで出力する必要があるのです．  
まずはテキストデータを出力するフォルダ(ディレクトリ, directory)を用意しましょう．フォルダ名はなんでもいいですが，`kuchikomi`とします
```{r eval = FALSE}
# フォルダの作成
dir.create("kuchikomi")
```

そしてこの`kuchikomi`フォルダにテキストデータを`.txt`形式で格納します．
```{r}
writeLines(text = use_series(df_jln, kuchikomi), con = "./kuchikomi/kuchikomi.txt")
```

ここでは`magrittr::use_series()`を使用しています．データフレームの形をとっていた口コミデータを一つのデータに直すイメージですね．  
また，ドット`.`は現在の作業場(ディレクトリ)を表します．相対パスですね．`kuchikomi`フォルダに`kuchikomi.txt`というファイルを作成するコマンドになっています．  

## 1.3 頻度表
ようやく準備ができました．さっそく分析に入っていきましょう．  
まずは頻度表を作成します．単語ごとの出現回数をカウントしたものですね．頻度表の作成には`RMeCab::RMeCabFreq()`という関数を使います．オブジェクト名は`df_freq`としておきましょう．再び相対パスを使ってファイルを指定します．

```{r}
df_freq <- RMeCabFreq("./kuchikomi/kuchikomi.txt")

# 確認
df_freq %>% head()
```


`Term`, `Info1`, `Info2`, `Freq`はそれぞれ，単語，品詞(大分類), 品詞(小分類), 出現頻度を表します．
```{r}
# 観測数
nrow(df_freq)
```
観測数が多いので，ここでは特に重要と思われる単語のみに絞ります．品詞は名詞，形容詞，動詞の三つ，頻度は40以上とし，あまり意味のない単語は削除したものを`df_freq2`とします．
```{r}
df_freq2 <- df_freq %>% 
  filter(Info1 %in% c("名詞", "形容詞", "動詞")) %>% 
  filter(Freq >= 40) %>% 
  filter(!Term %in% c("いる", "ある", "の", "なる", "できる", "する")) 

# 確認
df_freq2 %>% head()
```

### 1.3.1 棒グラフ
頻度表を作るだけだとデータがどんな意味を持っているか分かりづらいです．可視化してみましょう．2通り試してみます．まずは`ggplot2::ggplot()`を使って棒グラフを作成します.
```{r}
df_freq2 %>% 
  mutate(Term = reorder(Term, Freq)) %>% 
  ggplot(aes(Term, Freq)) + geom_col() + coord_flip()
```
上のコマンドは，`mutate()`内の`reorder`で`Term`を頻度`Freq`の大きい順に並べなおし，`ggplot()`で軸を決め，`geom_col()`で棒グラフを作成し，`coord_flip()`でxy軸をひっくり返してます．  


### 1.3.2 ワードクラウド
棒グラフも少し見づらい気がします．今度は`wordcloud::wordcloud()`を使ってワードクラウドを作ってみましょう．
```{r}
wordcloud(df_freq2$Term, df_freq2$Freq, color = brewer.pal(8, "Dark2"))
```

最初の引数で単語，次の引数で頻度を指定することで，頻度が多い単語を大きく描いています．また，`color = `で色分けしています．  
ずいぶん見やすくなりましたね．この宿は「露天風呂」がついていて，「食事」が「美味しい」のだろうとなんとなく想像できます．一方で「残念」という単語が見られるように，ネガティブな意見もあるようです．



# 2. 共起語
続いて，共起関係についてみていきます．  
共起(collocation)とは，ある単語が特定の別の単語と隣接して現れることです．共起関係を見ることで文脈が浮かび上がってきます．

## 2.1 Nグラムの作成
共起関係を見るためにNグラムを作成します．Nグラム(N-gram)とは単語の連なり(と頻度)を示した表のことです．  
Nグラムの作成には`RMeCab::NgramDF()`を使います．
```{r}
df_ngram <- NgramDF(filename = "./kuchikomi/kuchikomi.txt", type = 1, N = 2, 
                    pos = c("名詞", "形容詞", "動詞"))

# 確認
df_ngram %>% head()
```

`type = 1`は単語(正確には形態素)単位で区切ることを指定しています．`N = 2`で連なりの長さを2語にしています．また，`pos =`で品詞を指定しています．  
`head()`を見ると`!`などのあまり意味のない語が含まれているようなので，データを絞ります．
```{r}
df_ngram2 <- df_ngram %>% 
  filter(Freq >= 7) %>% 
  filter(! Ngram1 %in% c("いる", "ある", "の", "なる", "できる", "する")) %>% 
  filter(! Ngram2 %in% c("いる", "ある", "の", "なる", "できる", "する"))

# 確認
df_ngram2 %>%  head()
```

## 2.2 共起ネットワーク
この共起関係を可視化してみましょう．ここではパッケージ`{igraph}`を使います．  
まず，`igraph::graph.data.frame()`でNグラムを`igraph`型にします．これを`plor()`で描出します
```{r}
nw_graph <- graph.data.frame(df_ngram2)
plot(nw_graph)
```
ちょっと見づらいですね．ノードの大きさに頻度でウェイトをつけてみましょう．
```{r}
plot(nw_graph, vertex.size = df_ngram2$Freq*0.45)
```

`Freq`に`0.45`を掛けているのは単純に見やすくするためです．  
ちなみに，共起関係のようなネットワークを分析する際は様々なウェイトの付け方があり，実は頻度はあまり適切でないかもしれません．興味のある人は「ネットワーク分析」や「中心性」，「有向グラフ」などで調べてみてください．


# 3. 対応分析

今までテキストデータだけに注目して分析してきました．でも，せっかく年齢や性別といった属性情報も入手出来ているので，活用したいですよね．その一つの方法として対応分析(Correspondence Analysis)があります．

## 3.1 サンプルの整理
ここでもう一度`summary()`を見てみます．

```{r}
df_jln %>% 
  mutate_at(.vars = vars(gender, age), 
            .funs = as.factor) %>% 
  summary()
```

性別に関しては男女ともそれなりの数ですが，年齢に関しては70代と80代がほとんどいませんね．性別や年齢という軸でサンプルを区切る対応分析では，こうした極端にサイズの小さいサブサンプルは分析対象から外した方がよさそうです．
```{r}
df_jln2 <- df_jln %>% 
  filter(! age %in% c("70代", "80代"))

# 確認
df_jln2 %>% 
  mutate_at(.vars = vars(gender, age), 
            .funs = as.factor) %>% 
  summary()
```
## 3.2 .txtファイルの作成

対応分析でもいままでと同様に`.txt`ファイルを使用した分析を行うのですが，性別や年齢をごとにサンプルを区別するので，それらファイル名に反映させます．  
まずはサンプルの年齢をリストアップします．

```{r}
# 年齢リストの作成
age_list <- df_jln2 %>% 
  use_series(age) %>% 
  as.factor() %>% 
  levels()
```

テキストデータを各年齢の各性別で分けた`.txt`ファイルを格納するためのフォルダ(ディレクトリ)も用意しておきましょう．
```{r eval=FALSE}
dir.create("fmage")
```


`purrr::map()`を使って`.txt`ファイルを作成していきます．`purrr::map()`は指定したデータに対して指定した関数を当てはめるコマンドです．`{purrr}`パッケージは`{tidyverse}`に入っています．
まずは女性のサンプルから．`age_list`の順番に，`df_jln2`内の口コミデータを抽出し，それぞれ`F + (年齢).txt`として出力します．
```{r}
## 女性の20代~60代
age_list %>% 
  map(
    ~ filter(df_jln2, age == .x, gender == "女性") %>% 
      {
        tmp <- use_series(data = ., kuchikomi) %>% 
          as.character()
        writeLines(text = tmp, con = paste0("./fmage/F", (2:6)[age_list == .x], "0.txt"))
      }
  )
```
ティルダ`~`は関数を表します．「年齢」が与えられたときにその年齢の口コミデータを出力する関数を`~`以降で定義しています．その関数を`%>%`によって引き渡された`age_list`に当てはめているのです．  
男性サンプルについても同様に行います．

```{r}
## 男性の20代~60代
age_list %>% 
  map(
    ~filter(df_jln2, age == .x, gender == "男性") %>% 
      {
        tmp <- use_series(data = ., kuchikomi) %>% 
          as.character()
        writeLines(text = tmp, con = paste0("./fmage/M", (2:6)[age_list == .x], "0.txt"))
      }
  )
```



## 3.3 単語文書行列の作成
`fmage`フォルダにサンプルが格納されたので，これを使って分析します．複数のテキストデータについて，各ファイルに各語がどのくらいの頻度で出現したのかをまとめたデータフレームは`RMeCab::docDF()`で作成します．これは「単語文書行列」と呼ばれるものです．行が単語，列が文書を表しているのでそのままですね．
```{r}
df_fmage <- docDF("fmage", type = 1, pos = c("名詞", "形容詞", "動詞"))

# 確認
df_fmage %>% head()
```

今までと同様に，あまり意味のない単語は除外して分析しやすくしましょう．
```{r}
df_fmage2 <- df_fmage %>% 
  filter(POS2 %in% c("一般", "固有", "自立")) %>% 
  filter(! TERM %in% c("ある", "いう", "いる", "する", "できる", "なる", "思う"))
```

単語を選別し終えたので，品詞の情報はもういらないのです．また，行名を単語そのものにすることで列名を文書だけにしましょう．
```{r}
# 行名を単語にする
rownames(df_fmage2) <- df_fmage2$TERM

# 不要な列を削除
df_fmage2 <- df_fmage2 %>% 
  select(-(1:3)) 

# 確認
df_fmage2 %>% head()
```

また，出現頻度の少ない単語は重要でないと思われるので，削除しましょう．出現頻度の合計は`rowSums()`で男女20代~60代の出現頻度を合計することで算出できます．出現頻度の合計は12以上に絞りましょう．
```{r}
# 頻度で選別
df_fmage3 <- df_fmage2 %>%
  mutate(SUMS = rowSums(.)) %>% 
  filter(SUMS >= 12) %>% 
  select(- SUMS)

# 確認
df_fmage3 %>% rownames()
```


## 3.4 バイプロット
出来上がったサンプルを使って対応分析を行います．対応分析(Correspondence Analysis; CA)を行うコマンドは`FactoMineR::CA()`です．この出力結果を`factoextra::fviz_ca_biplot()`で描出します．この可視化はバイプロット(biplot)と呼ばれています．
```{r}
ca_fmage3 <- CA(df_fmage3, graph = FALSE)
fviz_ca_biplot(ca_fmage3)
```
近いところにプロットされているほど関係があると解釈できます．例えば，「子供」について言及しているのは30代の男女が多く，「客席」や「露天」に言及しているのは20代男性が多いという関係がみれます．  
厳密な解釈をするためには線形代数の知識が必要でちょっと複雑なのでここでは割愛します．興味のある人は特異値分解をさらいつつ[英語版wiki](https://en.wikipedia.org/wiki/Correspondence_analysis)を参照するとよいでしょう．特異値分解については[この記事](https://qiita.com/kidaufo/items/0f3da4ca4e19dc0e987e)が分かりやすいです．


# 4. 回帰分析への応用

ここからはテキストマイニングの結果を計量経済学的にどう活用していくかについてお話したいとおもいます．  
（完全にぼくオリジナルのアイディアなのでおかしいところがあるかも）

## 4.0 モデル: キーワードの出現頻度からレビュー(★の数)を予測する  
間に合わなかったです，すみません...  

## 4.1 すべての観測主体に対して口コミデータを.txt形式で出力  
xxx  

## 4.2 単語文書行列から「文書単語行列」を作成  
xxx  

## 4.3 レビューや属性情報と文書単語行列をマージ  
xxx  

## 4.4 回帰分析  
xxx  
