---
title: "2021年度 サブゼミ 第13回 Webスクレイピング"
author: "Yoshinari Namba"
date: "2021/8/11"
output: 
  github_document:
    pandoc_args: --webtex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 0. イントロダクション
今日のAgendaは以下の3つです．  
1. スクレイピングの概要  
2. Rでの実装  
3. グループワーク  

自力でWebからデータを取ってこれるようになることが今日の目標です！


# 1. スクレイピングの概要

## スクレイピングとは?
スクレイピングはWeb上のデータを自動で取得する方法です．  
欲しいデータの量が少ないときはコピペ・ダウンロードすれば良いのですが，膨大なデータが必要な場合は手作業でやると大変です．そこでスクレイピングを使います．


## 必要な知識
Webからデータを取ってくるので，Webサイトを構成するコード(HTML・CSS)をある程度知っておく必要があります.  
- HTML: Webページの枠組みを作る   
- CSS: HTML作った枠組みに肉付け(文字や配色の詳細設定)を行う

今日は深入りしないでおきます．とりあえず，HTMLが「大見出し→小見出し→本文」のような階層構造を持っていることだけ頭に入れておいて下さい！

## 注意点
サーバーを攻撃していると見なされると大変なことになります。データを繰り返し取得する際には適切な時間を空けましょう

## スクレイピングの手順
スクレイピングはざっくり以下の手順で行います(小澤さんのQiita記事)．  
1. 欲しいデータが載っているサイトを探す  
2. 欲しいデータがそのサイトのどの部分に記載されているかを把握する @ブラウザ  
3. Rでコードを書いてデータを取得する @R  

以降はこの手順で実装していきます！

# 2. Rでの実装

## 2-1. 欲しいデータが載っているサイトを探す
PC周辺機器の口コミを分析するケースを考えてみます．口コミはいろんなサイトに載っていると思いますが，ここでは楽天のレビューを使うことにしましょう．

## 2-2. 欲しいデータがそのサイトのどの部分に記載されているかを把握する @ブラウザ

### HTMLコードを開く
次のリンクから楽天のレビューページを開いてください．  
- https://review.rakuten.co.jp/search/-/100026/cu1001-d0/  

開けたら右クリックをして「検証」を選択してください (Edgeユーザーは「開発者ツールで調査する」)．

### 欲しいデータがHTML上のどの階層に位置しているのかを確認
左上の矢印マークを押して，欲しいデータが載っている場所をクリックしてください．  
ハイライトされているコードの中に「class = “...”」という部分が見つかると思います．  
[おまけ] Google Chromeの拡張機能Selector Gadgetを使う

## 2-3. Rでコードを書いてデータを取得する @R
コーディングは以下の手順で行います  
- Step1: 準備  
- Step2: 1つページからデータを取得  
- Step3: Step2を利用して2ページ目以降からデータを自動的に取得  

### Step1: 準備
RStudioを開いて右上のタブの File -> New Project から新規プロジェクトを作成しておいてください．  

#### パッケージのインストール
スクレイピングには`rvest`というパッケージを使用します．  
あとで使う`tidyverse`も合わせて先に必要なパッケージを呼び出しましょう．
```{r eval=FALSE}
# パッケージのインストール
install.packages("rvest")

# パッケージの呼び出し
library(rvest)
library(tidyverse)
```
まだインストールしていない人は`install.packages("tidyverse")`から実行するようにしてください．

```{r include=FALSE}
library(rvest)
library(tidyverse)
```


#### ウェブサイトを指定する  
まずはサイトのurlを指定します．htmlを読み込むには`rvest::read_html()`を使います．
```{r eval=FALSE}
# 対象のサイトのURLを指定
url <- read_html("https://review.rakuten.co.jp/search/-/100026/cu1001-d0/")
```

実行してみるとエラーが出てしまいます．エンコ―ディングの問題で読み込めないみたいです．正しいエンコードを知るには`guess_encoding()`を使います [^1]．
```{r}
guess_encoding("https://review.rakuten.co.jp/search/-/100026/cu1001-d0/")
```
このサイトは`EUC-JP`というエンコードが信憑性が高そうです．このエンコードで再度読み込みます．

```{r}
# エンコードを指定して再度読み込み
url <- read_html("https://review.rakuten.co.jp/search/-/100026/cu1001-d0/", 
                 encoding = "EUC-JP")
```

[^1]: `guess_encoding`は`tidyverse`の中にある`readr`というパッケージの中にあります．


### Step2: 1つのページからデータを取得
urlから欲しいデータを取ってくるには`html_nodes()`というコマンドを使用します．`()`内には先ほどブラウザで確認したパス(HTML上の位置)を指定します．また，取ってきたデータをテキストの形式で出力するために`html_text()`を使います．

```{r}
# データの取得
reviews <- url %>% 
  html_nodes(".ratCustomAppearTarget") %>%  # 欲しい情報の位置を指定
  html_text()                               # テキスト形式で出力
```

ちゃんとデータが取れているか最初の10行を確認してみましょう．

```{r}
# 確認
head(reviews, n = 10)
```

`\n`が何度も出てきて気持ち悪いですね... `str_replace_all()`を使って消してみましょう．  (ちなみに`\n`は改行を表しています．)

```{r}
# \n を削除
reviews <- reviews %>% 
  str_replace_all(pattern = '\n', replacement = '')

# 確認
head(reviews, n = 10)
```
とりあえずこのページから必要なデータが取れましたね．

### Step3: 2ページ目以降からデータを自動的に取得
1ページで終わりならコピペとそれほど労力はかわらないかもしれません．でも，数十，数百ページからデータを取ろうとするとスクレイピングの方が圧倒的に早いです．  
ここからはStep2で書いたコードを使って複数ページからデータを取得するコードを書きます

#### URLの規則を把握する
ページを変えるとURLも変わるため，指定するURLを変えなければいけません．  
URLの変化の仕方が規則的ならば複数ページからデータを取る作業を自動化することができます．  
実際に2ページ目を見てみると末尾が`....-d0-p2/`，3ページ目は`...-d0-p3/`となっています．
```{r}
# データを取得
for(i in 2:10){
  url_tmp <- paste0("https://review.rakuten.co.jp/search/-/100026/cu1001-d0-p", i, "/") %>% # iページ目のURLを作成
    read_html(encoding = "EUC-JP")                    # ページの読み込み
  reviews_tmp <- url_tmp %>% 
    html_nodes(".ratCustomAppearTarget") %>%          # データの位置を指定 
    html_text() %>%                                   # テキスト形式で出力
    str_replace_all(pattern = "\n", replacement = "") # 文字を整える
  reviews <- c(reviews, reviews_tmp)                  # i-1ページ目までのデータと結合
  Sys.sleep(1)                                        # 1秒時間を空ける
}

# 確認
reviews[201:210]
```

# 3. グループワーク
今まで学んだことを実践してみましょう． 
次のサイトから東京都港区のコンビニのデータを取得してみてください．  
- https://www.homemate-research-convenience-store.com/13103/  

欲しいデータは  
- 店舗名  
- 住所  

とします．時間に余裕があったらこれらを1つのデータフレームにまとめてみてください．また，
- 口コミ数  

もGetしてみてください．


# < 参考 > 
- 小澤さん「RでのWeb スクレイピング入門」(https://qiita.com/Tom-tom-tom/items/998e8282d013fb218490)  
- 村松ほか, 「スクレイピングによるデータ収集」, 技術評論社, 『RユーザーのためのRStudio「実践」入門』，第2章  
